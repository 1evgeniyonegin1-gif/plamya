"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ Telegram-–∫–∞–Ω–∞–ª–∞ NL International
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï –ø–æ–ª–µ–∑–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è RAG –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:
1. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º (—Å—Ö–µ–º—ã –ø—Ä–∏—ë–º–∞)
2. –û–±—É—á–∞—é—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç (—É—Ä–æ–∫–∏, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
3. –ü—Ä–∏–º–µ—Ä—ã –ø–æ—Å—Ç–æ–≤ (—à–∞–±–ª–æ–Ω—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
4. –ò—Å—Ç–æ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –¥–æ/–ø–æ—Å–ª–µ)
5. FAQ –∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è
6. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö
7. –ë–∏–∑–Ω–µ—Å –∏ –º–æ—Ç–∏–≤–∞—Ü–∏—è

–í–ê–ñ–ù–û: –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –Ω–∞:
- EVERGREEN (–≤–µ—á–Ω—ã–µ) ‚Äî –±–µ–∑ —Ü–µ–Ω, –∞–∫—Ü–∏–π, –¥–∞—Ç ‚Äî –¥–ª—è RAG –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
- TIME_SENSITIVE (–≤—Ä–µ–º–µ–Ω–Ω—ã–µ) ‚Äî —Å —Ü–µ–Ω–∞–º–∏, –∞–∫—Ü–∏—è–º–∏ ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ—Å—Ç–æ–≤
"""

import json
import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import sys
import io

# –§–∏–∫—Å –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')


class TelegramKnowledgeExtractor:
    def __init__(self, export_path: str):
        self.export_path = Path(export_path)
        self.messages = []
        self.extracted_content = defaultdict(list)

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        self.min_lengths = {
            'recommendations': 200,  # –°—Ö–µ–º—ã –ø—Ä–∏—ë–º–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏
            'training': 150,         # –û–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–æ—Ä–æ—á–µ
            'post_examples': 200,    # –ü—Ä–∏–º–µ—Ä—ã –ø–æ—Å—Ç–æ–≤ - —Å—Ä–µ–¥–Ω–∏–µ
            'success_stories': 100,  # –ò—Å—Ç–æ—Ä–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Å —Ñ–æ—Ç–æ
            'faq': 100,              # –í–æ–ø—Ä–æ—Å—ã-–æ—Ç–≤–µ—Ç—ã
            'products': 150,         # –û–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤
            'business': 150,         # –ë–∏–∑–Ω–µ—Å –∫–æ–Ω—Ç–µ–Ω—Ç
            'motivation': 100,       # –ú–æ—Ç–∏–≤–∞—Ü–∏—è
        }

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        self.category_patterns = {
            'recommendations': {
                'keywords': [
                    r'—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü', r'—Å—Ö–µ–º–∞ –ø—Ä–∏—ë–º', r'–∫–∞–∫ –ø—Ä–∏–Ω–∏–º–∞—Ç—å', r'–∫—É—Ä—Å –ø—Ä–∏—ë–º–∞',
                    r'–ø—Ä–æ–≥—Ä–∞–º–º–∞.*–ø–æ—Ö—É–¥–µ–Ω', r'–ø—Ä–æ–≥—Ä–∞–º–º–∞.*–∑–¥–æ—Ä–æ–≤', r'–∫–æ–º–ø–ª–µ–∫—Å.*–ø—Ä–∏—ë–º',
                    r'‚úÖ.*:', r'–ø—Ä–∏–Ω–∏–º–∞—Ç—å.*—É—Ç—Ä', r'–ø—Ä–∏–Ω–∏–º–∞—Ç—å.*–≤–µ—á–µ—Ä', r'–¥–æ–∑–∏—Ä–æ–≤–∫',
                    r'–ø–∏—Ç—å.*—Ä–∞–∑.*–¥–µ–Ω—å', r'–∫–∞–ø—Å—É–ª.*–¥–µ–Ω—å', r'—Å–∞—à–µ.*–¥–µ–Ω—å',
                    r'–ª–∏—à–Ω–∏–π –≤–µ—Å.*:', r'–¥–∏–∞–±–µ—Ç.*:', r'—Å—É—Å—Ç–∞–≤—ã.*:', r'–∏–º–º—É–Ω–∏—Ç–µ—Ç.*:',
                    r'–¥–ª—è.*—Ä–µ–∫–æ–º–µ–Ω–¥—É', r'–ø—Ä–∏.*—Ä–µ–∫–æ–º–µ–Ω–¥—É',
                ],
                'negative': [r'—Ä–µ—Ü–µ–ø—Ç.*–∫—É–ª–∏—á', r'—Ä–µ—Ü–µ–ø—Ç.*—Ç–æ—Ä—Ç', r'–≥–æ—Ç–æ–≤.*–±–ª—é–¥'],
            },
            'training': {
                'keywords': [
                    r'—É—Ä–æ–∫', r'–æ–±—É—á–µ–Ω', r'–≤–µ–±–∏–Ω–∞—Ä', r'—à–∫–æ–ª', r'–∫—É—Ä—Å',
                    r'gostories', r'–∏–Ω—Å—Ç—Ä—É–∫—Ü', r'–∫–∞–∫.*–¥–µ–ª–∞—Ç—å', r'–∫–∞–∫.*—Å–æ–∑–¥–∞—Ç—å',
                    r'—à–∞–≥.*\d', r'—ç—Ç–∞–ø.*\d', r'–ø—Ä–∞–≤–∏–ª.*\d',
                    r'–æ—à–∏–±–∫.*–∫–æ—Ç–æ—Ä', r'—Å–µ–∫—Ä–µ—Ç', r'–ª–∞–π—Ñ—Ö–∞–∫',
                    r'–ø—Ä–æ—Ñ–∏–ª.*instagram', r'stories', r'reels', r'–∫–æ–Ω—Ç–µ–Ω—Ç.*–ø–ª–∞–Ω',
                ],
                'negative': [],
            },
            'post_examples': {
                'keywords': [
                    r'–ø—Ä–∏–º–µ—Ä –ø–æ—Å—Ç–∞', r'–ø–æ—Å—Ç.*–ø—Ä–∏–º–µ—Ä', r'—à–∞–±–ª–æ–Ω', r'–æ–±—Ä–∞–∑–µ—Ü –ø–æ—Å—Ç–∞',
                    r'–∫–æ–ø–∏—Ä—É–π', r'–∏—Å–ø–æ–ª—å–∑—É–π.*—Ç–µ–∫—Å—Ç', r'–≥–æ—Ç–æ–≤—ã–π.*–ø–æ—Å—Ç',
                    r'^–¥–æ\s*-?\d+\s*–∫–≥', r'–º–∏–Ω—É—Å.*–∫–≥.*–∑–∞', r'—Ä–µ–∑—É–ª—å—Ç–∞—Ç.*–¥–Ω',
                ],
                'negative': [],
            },
            'success_stories': {
                'keywords': [
                    r'—Ä–µ–∑—É–ª—å—Ç–∞—Ç', r'–ø–æ—Ö—É–¥–µ–ª', r'—Å–±—Ä–æ—Å–∏–ª', r'–º–∏–Ω—É—Å.*–∫–≥',
                    r'–¥–æ/–ø–æ—Å–ª–µ', r'–¥–æ –∏ –ø–æ—Å–ª–µ', r'–±—ã–ª–æ.*—Å—Ç–∞–ª–æ',
                    r'–∏—Å—Ç–æ—Ä–∏—è.*—É—Å–ø–µ—Ö', r'–º–æ–π.*–ø—É—Ç—å', r'–∫–∞–∫ —è',
                    r'–±–ª–∞–≥–æ–¥–∞—Ä.*nl', r'–±–ª–∞–≥–æ–¥–∞—Ä.*energy', r'–∏–∑–º–µ–Ω–∏–ª.*–∂–∏–∑–Ω—å',
                    r'-\d+\s*–∫–≥', r'—É—à–ª–æ.*–∫–≥', r'—Å–∫–∏–Ω—É–ª',
                ],
                'negative': [],
            },
            'faq': {
                'keywords': [
                    r'–≤–æ–ø—Ä–æ—Å.*–æ—Ç–≤–µ—Ç', r'—á–∞—Å—Ç–æ.*—Å–ø—Ä–∞—à–∏–≤–∞', r'faq',
                    r'–ø–æ—á–µ–º—É.*\?', r'–∫–∞–∫.*\?', r'—á—Ç–æ.*–µ—Å–ª–∏.*\?',
                    r'–º–∏—Ñ—ã.*–æ', r'–ø—Ä–∞–≤–¥–∞.*–æ', r'–∑–∞–±–ª—É–∂–¥–µ–Ω',
                    r'—ç—Ç–æ.*—Ä–∞–∑–≤–æ–¥', r'—ç—Ç–æ.*–ø–∏—Ä–∞–º–∏–¥', r'—ç—Ç–æ.*–º–ª–º',
                    r'–≤–æ–∑—Ä–∞–∂–µ–Ω', r'–æ—Ç–≤–µ—á–∞—Ç—å.*–Ω–∞',
                ],
                'negative': [],
            },
            'products': {
                'keywords': [
                    r'energy\s*diet', r'greenflash', r'green\s*flash',
                    r'collagen', r'–∫–æ–ª–ª–∞–≥–µ–Ω', r'biodrone', r'–±–∏–æ–¥—Ä–æ–Ω',
                    r'draineffect', r'–¥—Ä–∞–π–Ω', r'occuba', r'–æ–∫–∫—É–±–∞',
                    r'enerwood', r'—ç–Ω–µ—Ä–≤—É–¥', r'3d\s*slim', r'—Å–ª–∏–º',
                    r'omega', r'–æ–º–µ–≥–∞', r'–≤–∏—Ç–∞–º–∏–Ω', r'–±–∞–¥',
                    r'—Å–æ—Å—Ç–∞–≤.*–ø—Ä–æ–¥—É–∫—Ç', r'–∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç', r'—Å–≤–æ–π—Å—Ç–≤.*–ø—Ä–æ–¥—É–∫—Ç',
                ],
                'negative': [],
            },
            'business': {
                'keywords': [
                    r'–±–∏–∑–Ω–µ—Å', r'–ø–∞—Ä—Ç–Ω—ë—Ä', r'–ø–∞—Ä—Ç–Ω–µ—Ä', r'—Å—Ç—Ä—É–∫—Ç—É—Ä',
                    r'–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü', r'–º–∞—Ä–∫–µ—Ç–∏–Ω–≥.*–ø–ª–∞–Ω', r'–∑–∞—Ä–∞–±–æ—Ç', r'–¥–æ—Ö–æ–¥',
                    r'—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü.*nl', r'—Å—Ç–∞—Ç—å.*–ø–∞—Ä—Ç–Ω—ë—Ä', r'–∫–æ–º–∞–Ω–¥',
                    r'–ª–∏–¥–µ—Ä', r'–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫', r'—Å–ø–æ–Ω—Å–æ—Ä', r'–º–µ–Ω–µ–¥–∂–µ—Ä',
                    r'—Ç–æ–≤–∞—Ä–æ–æ–±–æ—Ä–æ—Ç', r'pv', r'–±–æ–Ω—É—Å', r'—á–µ–∫',
                ],
                'negative': [],
            },
            'motivation': {
                'keywords': [
                    r'–º–æ—Ç–∏–≤–∞—Ü', r'–≤–¥–æ—Ö–Ω–æ–≤–µ–Ω', r'–≤–µ—Ä—å.*—Å–µ–±', r'—Ç—ã.*—Å–º–æ–∂–µ—à—å',
                    r'–Ω–µ —Å–¥–∞–≤–∞–π', r'—Ü–µ–ª—å', r'–º–µ—á—Ç', r'–¥–æ—Å—Ç–∏–≥', r'—É—Å–ø–µ—Ö',
                    r'–∏–∑–º–µ–Ω–∏.*–∂–∏–∑–Ω—å', r'–Ω–æ–≤–∞—è.*–∂–∏–∑–Ω—å', r'–Ω–∞—á–Ω–∏.*—Å–µ–≥–æ–¥–Ω',
                    r'–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç', r'–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª', r'—Ä–æ—Å—Ç',
                ],
                'negative': [],
            },
        }

        # –ü—Ä–æ–¥—É–∫—Ç—ã NL –¥–ª—è —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.nl_products = [
            'Energy Diet', 'Greenflash', 'Green Flash', 'Collagen', '–ö–æ–ª–ª–∞–≥–µ–Ω',
            'BioDrone', '–ë–∏–æ–¥—Ä–æ–Ω', 'DrainEffect', '–î—Ä–∞–π–Ω', 'Occuba', '–û–∫–∫—É–±–∞',
            'Enerwood', '–≠–Ω–µ—Ä–≤—É–¥', '3D Slim', 'Omega', '–û–º–µ–≥–∞', 'BioTuning',
            'BioSetting', 'Herbal Tea', 'Detox', '–î–µ—Ç–æ–∫—Å', 'Be Loved',
        ]

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Ü–µ–Ω—ã, –∞–∫—Ü–∏–∏, –¥–∞—Ç—ã)
        self.time_sensitive_patterns = [
            r'\d+\s*‚ÇΩ',                    # 1500‚ÇΩ
            r'\d+\s*—Ä—É–±',                  # 1500 —Ä—É–±–ª–µ–π
            r'\d+\s*rub',                  # 1500 rub
            r'—Ü–µ–Ω–∞\s*:?\s*\d+',            # —Ü–µ–Ω–∞: 1500
            r'—Å—Ç–æ–∏—Ç\s*\d+',                # —Å—Ç–æ–∏—Ç 1500
            r'–≤—Å–µ–≥–æ\s*\d+',                # –≤—Å–µ–≥–æ 1500
            r'–∞–∫—Ü–∏—è',                      # –∞–∫—Ü–∏—è
            r'—Å–∫–∏–¥–∫[–∞–∏]',                  # —Å–∫–∏–¥–∫–∞/—Å–∫–∏–¥–∫–∏
            r'–ø—Ä–æ–º–æ',                      # –ø—Ä–æ–º–æ
            r'—Ç–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è',             # —Ç–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è
            r'–¥–æ –∫–æ–Ω—Ü–∞ (–Ω–µ–¥–µ–ª–∏|–º–µ—Å—è—Ü–∞|–≥–æ–¥–∞)',  # –¥–æ –∫–æ–Ω—Ü–∞ –º–µ—Å—è—Ü–∞
            r'—Ä–∞—Å–ø—Ä–æ–¥–∞–∂',                  # —Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞
            r'—Å–ø–µ—Ü–∏–∞–ª—å–Ω\w+ —Ü–µ–Ω',           # —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞
            r'–≤–º–µ—Å—Ç–æ\s*\d+',               # –≤–º–µ—Å—Ç–æ 2000
            r'-\d+%',                      # -20%
            r'\d+%\s*—Å–∫–∏–¥–∫',               # 20% —Å–∫–∏–¥–∫–∞
        ]

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ—Ç–æ—Ä—ã–µ –í–°–ï–ì–î–ê –≤–µ—á–Ω—ã–µ (–Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ü–µ–Ω—ã)
        self.always_evergreen = ['training', 'faq', 'motivation']

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.check_for_time_sensitive = ['products', 'recommendations', 'business', 'success_stories', 'post_examples']

    def load_export(self) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —ç–∫—Å–ø–æ—Ä—Ç–∞ Telegram"""
        print("=" * 60)
        print("–ó–ê–ì–†–£–ó–ö–ê –≠–ö–°–ü–û–†–¢–ê TELEGRAM")
        print("=" * 60)

        result_json = self.export_path / 'result.json'

        if not result_json.exists():
            print(f"[ERROR] –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {result_json}")
            return False

        with open(result_json, 'r', encoding='utf-8') as f:
            data = json.load(f)

        self.channel_name = data.get('name', 'Unknown')
        self.messages = data.get('messages', [])

        print(f"–ö–∞–Ω–∞–ª: {self.channel_name}")
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(self.messages)}")

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏—è (–Ω–µ —Å–µ—Ä–≤–∏—Å–Ω—ã–µ)
        self.messages = [m for m in self.messages if m.get('type') == 'message']
        print(f"–°–æ–æ–±—â–µ–Ω–∏–π (–±–µ–∑ —Å–µ—Ä–≤–∏—Å–Ω—ã—Ö): {len(self.messages)}")

        return True

    def get_text(self, msg: dict) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        text = msg.get('text', '')
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            parts = []
            for item in text:
                if isinstance(item, str):
                    parts.append(item)
                elif isinstance(item, dict):
                    parts.append(item.get('text', ''))
            return ' '.join(parts)
        return ''

    def categorize_message(self, msg: dict) -> list:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        text = self.get_text(msg).lower()
        if not text.strip():
            return []

        categories = []

        for category, patterns in self.category_patterns.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∏—Å–∫–ª—é—á–µ–Ω–∏—è)
            skip = False
            for neg_pattern in patterns.get('negative', []):
                if re.search(neg_pattern, text, re.IGNORECASE):
                    skip = True
                    break

            if skip:
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in patterns['keywords']:
                if re.search(pattern, text, re.IGNORECASE):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
                    if len(text) >= self.min_lengths.get(category, 100):
                        categories.append(category)
                    break

        return categories

    def extract_products_mentioned(self, text: str) -> list:
        """–ù–∞—Ö–æ–¥–∏—Ç —É–ø–æ–º—è–Ω—É—Ç—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã NL"""
        products = []
        text_lower = text.lower()
        for product in self.nl_products:
            if product.lower() in text_lower:
                products.append(product)
        return list(set(products))

    def is_time_sensitive(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ü–µ–Ω—ã, –∞–∫—Ü–∏–∏)"""
        text_lower = text.lower()
        for pattern in self.time_sensitive_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return True
        return False

    def clean_prices_from_text(self, text: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç —Ü–µ–Ω—ã –∏ –∞–∫—Ü–∏–æ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è evergreen –≤–µ—Ä—Å–∏–∏"""
        cleaned = text

        # –£–¥–∞–ª—è–µ–º —Ü–µ–Ω—ã –≤ —Ä—É–±–ª—è—Ö
        cleaned = re.sub(r'\d+\s*‚ÇΩ', '[—Ü–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É]', cleaned)
        cleaned = re.sub(r'\d+\s*—Ä—É–±\w*', '[—Ü–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É]', cleaned)
        cleaned = re.sub(r'\d+\s*rub\w*', '[—Ü–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É]', cleaned, flags=re.IGNORECASE)

        # –£–¥–∞–ª—è–µ–º –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ "—Ü–µ–Ω–∞: XXXX"
        cleaned = re.sub(r'—Ü–µ–Ω–∞\s*:?\s*\d+\s*(‚ÇΩ|—Ä—É–±\w*)?', '[—Ü–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É]', cleaned, flags=re.IGNORECASE)

        # –£–¥–∞–ª—è–µ–º "—Å—Ç–æ–∏—Ç XXXX"
        cleaned = re.sub(r'—Å—Ç–æ–∏—Ç\s*\d+\s*(‚ÇΩ|—Ä—É–±\w*)?', '—Å—Ç–æ–∏—Ç [—Ü–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É]', cleaned, flags=re.IGNORECASE)

        # –£–¥–∞–ª—è–µ–º "–≤–º–µ—Å—Ç–æ XXXX"
        cleaned = re.sub(r'–≤–º–µ—Å—Ç–æ\s*\d+\s*(‚ÇΩ|—Ä—É–±\w*)?', '', cleaned, flags=re.IGNORECASE)

        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã —Å–∫–∏–¥–æ–∫
        cleaned = re.sub(r'-?\d+%\s*(—Å–∫–∏–¥–∫\w*)?', '', cleaned, flags=re.IGNORECASE)

        # –£–¥–∞–ª—è–µ–º "—Ç–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è", "–¥–æ –∫–æ–Ω—Ü–∞ –º–µ—Å—è—Ü–∞" –∏ —Ç.–¥.
        cleaned = re.sub(r'—Ç–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è\s*[-‚Äî]?\s*', '', cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r'–¥–æ –∫–æ–Ω—Ü–∞\s+(–Ω–µ–¥–µ–ª–∏|–º–µ—Å—è—Ü–∞|–≥–æ–¥–∞)\s*', '', cleaned, flags=re.IGNORECASE)

        # –£–¥–∞–ª—è–µ–º —Å–ª–æ–≤–∞ "–∞–∫—Ü–∏—è", "–ø—Ä–æ–º–æ", "—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞" –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
        cleaned = re.sub(r'üéâ?\s*–∞–∫—Ü–∏—è\s*üéâ?', '', cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r'–ø—Ä–æ–º–æ\s*:?\s*', '', cleaned, flags=re.IGNORECASE)

        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        cleaned = re.sub(r'  +', ' ', cleaned)
        cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)

        return cleaned.strip()

    def calculate_quality_score(self, msg: dict, text: str) -> int:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (0-100)"""
        score = 0

        # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (–º–∞–∫—Å 30 –±–∞–ª–ª–æ–≤)
        length = len(text)
        if length > 1000:
            score += 30
        elif length > 500:
            score += 25
        elif length > 300:
            score += 20
        elif length > 150:
            score += 10

        # –ù–∞–ª–∏—á–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–º–∞–∫—Å 20 –±–∞–ª–ª–æ–≤)
        if re.search(r'[‚úÖ‚úîÔ∏è‚òëÔ∏è]', text):
            score += 10
        if re.search(r'\d+[.)\-]', text):  # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            score += 10

        # –ù–∞–ª–∏—á–∏–µ —ç–º–æ–¥–∑–∏ (–ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω–æ–≥–æ –ø–æ—Å—Ç–∞) - –º–∞–∫—Å 10
        emoji_count = len(re.findall(r'[\U0001F300-\U0001F9FF]', text))
        if emoji_count > 3:
            score += 10
        elif emoji_count > 0:
            score += 5

        # –£–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ NL (–º–∞–∫—Å 15)
        products = self.extract_products_mentioned(text)
        score += min(len(products) * 5, 15)

        # –ù–∞–ª–∏—á–∏–µ —Ñ–æ—Ç–æ (–º–∞–∫—Å 10)
        if msg.get('photo'):
            score += 10

        # –ù–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫ (–º–∞–∫—Å 5)
        if 'http' in text.lower() or 't.me' in text.lower():
            score += 5

        return min(score, 100)

    def process_messages(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –∏—Ö"""
        print("\n" + "=" * 60)
        print("–ö–ê–¢–ï–ì–û–†–ò–ó–ê–¶–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê")
        print("=" * 60)

        stats = defaultdict(int)

        for idx, msg in enumerate(self.messages):
            if idx % 2000 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {idx}/{len(self.messages)}")

            text = self.get_text(msg)
            if not text.strip():
                continue

            categories = self.categorize_message(msg)
            if not categories:
                continue

            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            quality = self.calculate_quality_score(msg, text)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (score >= 25)
            if quality < 25:
                continue

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            time_sensitive = self.is_time_sensitive(text)

            # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å
            entry = {
                'id': msg.get('id'),
                'date': msg.get('date', ''),
                'text': text,
                'text_cleaned': self.clean_prices_from_text(text) if time_sensitive else text,
                'author': msg.get('from', msg.get('actor', '')),
                'quality_score': quality,
                'products_mentioned': self.extract_products_mentioned(text),
                'has_photo': bool(msg.get('photo')),
                'photo_path': msg.get('photo', ''),
                'categories': categories,
                'is_time_sensitive': time_sensitive,
            }

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–∞–∂–¥—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            for category in categories:
                self.extracted_content[category].append(entry)
                stats[category] += 1

        print("\n–ù–∞–π–¥–µ–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
        for category, count in sorted(stats.items(), key=lambda x: -x[1]):
            print(f"  {category}: {count}")

    def deduplicate_content(self):
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã (–ø–æ—Ö–æ–∂–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è)"""
        print("\n" + "=" * 60)
        print("–î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø")
        print("=" * 60)

        for category in self.extracted_content:
            entries = self.extracted_content[category]
            original_count = len(entries)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É (–ª—É—á—à–∏–µ –ø–µ—Ä–≤—ã–µ)
            entries.sort(key=lambda x: -x['quality_score'])

            # –£–¥–∞–ª—è–µ–º –ø–æ—Ö–æ–∂–∏–µ (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ —Å–æ–≤–ø–∞–¥–∞—é—Ç)
            seen_prefixes = set()
            unique_entries = []

            for entry in entries:
                prefix = entry['text'][:100].lower().strip()
                if prefix not in seen_prefixes:
                    seen_prefixes.add(prefix)
                    unique_entries.append(entry)

            self.extracted_content[category] = unique_entries

            removed = original_count - len(unique_entries)
            if removed > 0:
                print(f"  {category}: {original_count} -> {len(unique_entries)} (-{removed} –¥—É–±–ª–µ–π)")

    def save_results(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã"""
        print("\n" + "=" * 60)
        print("–°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print("=" * 60)

        output_dir = Path(__file__).parent.parent / 'content' / 'telegram_knowledge'
        output_dir.mkdir(exist_ok=True, parents=True)

        total_entries = 0

        for category, entries in self.extracted_content.items():
            if not entries:
                continue

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
            entries.sort(key=lambda x: -x['quality_score'])

            # –ë–µ—Ä—ë–º —Ç–æ–ø-100 –ª—É—á—à–∏—Ö –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            top_entries = entries[:100]

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
            json_path = output_dir / f'{category}.json'
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'category': category,
                    'total_found': len(entries),
                    'saved_top': len(top_entries),
                    'entries': top_entries,
                }, f, ensure_ascii=False, indent=2)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è RAG (—Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç—ã)
            txt_path = output_dir / f'{category}.txt'
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(f"# {category.upper()}\n")
                f.write(f"# –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ –∫–∞–Ω–∞–ª–∞: {self.channel_name}\n")
                f.write(f"# –î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {datetime.now().isoformat()}\n")
                f.write(f"# –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(top_entries)}\n")
                f.write("=" * 60 + "\n\n")

                for i, entry in enumerate(top_entries, 1):
                    f.write(f"--- #{i} | {entry['date']} | Score: {entry['quality_score']} ---\n")
                    if entry['products_mentioned']:
                        f.write(f"–ü—Ä–æ–¥—É–∫—Ç—ã: {', '.join(entry['products_mentioned'])}\n")
                    f.write(f"\n{entry['text']}\n\n")
                    f.write("-" * 40 + "\n\n")

            print(f"  {category}: {len(top_entries)} –∑–∞–ø–∏—Å–µ–π -> {json_path.name}")
            total_entries += len(top_entries)

        # –°–æ–∑–¥–∞—ë–º —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á—ë—Ç
        report = {
            'source': str(self.export_path),
            'channel_name': self.channel_name,
            'processed_date': datetime.now().isoformat(),
            'total_messages': len(self.messages),
            'categories': {}
        }

        for category, entries in self.extracted_content.items():
            report['categories'][category] = {
                'total_found': len(entries),
                'saved': min(len(entries), 100),
                'avg_quality': sum(e['quality_score'] for e in entries[:100]) / max(len(entries[:100]), 1),
            }

        report_path = output_dir / 'extraction_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\n–ò—Ç–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_entries} –∑–∞–ø–∏—Å–µ–π")
        print(f"–ü–∞–ø–∫–∞: {output_dir}")
        print(f"–û—Ç—á—ë—Ç: {report_path}")

        return output_dir

    def create_rag_documents(self, output_dir: Path):
        """–°–æ–∑–¥–∞—ë—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è RAG –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

        –†–∞–∑–¥–µ–ª—è–µ—Ç –Ω–∞:
        - EVERGREEN (–≤–µ—á–Ω—ã–µ) ‚Äî –¥–ª—è RAG, –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ —Ü–µ–Ω
        - PROMO (–ø—Ä–∏–º–µ—Ä—ã –ø–æ—Å—Ç–æ–≤ —Å —Ü–µ–Ω–∞–º–∏) ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        """
        print("\n" + "=" * 60)
        print("–°–û–ó–î–ê–ù–ò–ï RAG –î–û–ö–£–ú–ï–ù–¢–û–í (—Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º)")
        print("=" * 60)

        # –û—Å–Ω–æ–≤–Ω–∞—è –ø–∞–ø–∫–∞ RAG (evergreen –∫–æ–Ω—Ç–µ–Ω—Ç)
        rag_dir = Path(__file__).parent.parent / 'content' / 'knowledge_base' / 'from_telegram'
        rag_dir.mkdir(exist_ok=True, parents=True)

        # –ü–∞–ø–∫–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ—Å—Ç–æ–≤ —Å —Ü–µ–Ω–∞–º–∏ (–¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç-–±–æ—Ç–∞)
        promo_dir = Path(__file__).parent.parent / 'content' / 'knowledge_base' / 'promo_examples'
        promo_dir.mkdir(exist_ok=True, parents=True)

        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        title_map = {
            'recommendations': '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º NL International',
            'training': '–û–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã NL International',
            'post_examples': '–ü—Ä–∏–º–µ—Ä—ã –ø–æ—Å—Ç–æ–≤ NL International',
            'success_stories': '–ò—Å—Ç–æ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞ –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ NL International',
            'faq': '–ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã –æ NL International',
            'products': '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö NL International',
            'business': '–ë–∏–∑–Ω–µ—Å —Å NL International',
            'motivation': '–ú–æ—Ç–∏–≤–∞—Ü–∏—è –∏ –≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏–µ NL International',
        }

        created_evergreen = 0
        created_promo = 0

        for category, entries in self.extracted_content.items():
            if not entries:
                continue

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
            sorted_entries = sorted(entries, key=lambda x: -x['quality_score'])

            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ evergreen –∏ time_sensitive
            if category in self.always_evergreen:
                # –≠—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤—Å–µ–≥–¥–∞ –≤–µ—á–Ω—ã–µ
                evergreen_entries = sorted_entries[:30]
                promo_entries = []
            else:
                # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö ‚Äî —Ä–∞–∑–¥–µ–ª—è–µ–º
                evergreen_entries = [e for e in sorted_entries if not e['is_time_sensitive']][:30]
                promo_entries = [e for e in sorted_entries if e['is_time_sensitive']][:20]

            # === –°–û–ó–î–ê–Å–ú EVERGREEN –î–û–ö–£–ú–ï–ù–¢–´ ===
            if evergreen_entries:
                for chunk_idx in range(0, len(evergreen_entries), 10):
                    chunk = evergreen_entries[chunk_idx:chunk_idx + 10]
                    file_num = chunk_idx // 10 + 1

                    filename = f'telegram_{category}_evergreen_{file_num}.txt'
                    filepath = rag_dir / filename

                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(f"# {title_map.get(category, category)}\n\n")
                        f.write(f"–ò—Å—Ç–æ—á–Ω–∏–∫: –†–∞–±–æ—á–∏–π –∫–∞–Ω–∞–ª NL International\n")
                        f.write(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}\n")
                        f.write(f"–¢–∏–ø: EVERGREEN (–≤–µ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç)\n")
                        f.write(f"–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¶–µ–Ω—ã –∏ –∞–∫—Ü–∏–∏ –º–æ–≥—É—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è, —É—Ç–æ—á–Ω—è–π—Ç–µ –∞–∫—Ç—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n\n")
                        f.write("---\n\n")

                        for entry in chunk:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è evergreen
                            text_to_use = entry.get('text_cleaned', entry['text'])
                            f.write(f"{text_to_use}\n\n")
                            f.write("---\n\n")

                    created_evergreen += 1

            # === –°–û–ó–î–ê–Å–ú PROMO –î–û–ö–£–ú–ï–ù–¢–´ (–ø—Ä–∏–º–µ—Ä—ã –ø–æ—Å—Ç–æ–≤ —Å —Ü–µ–Ω–∞–º–∏) ===
            if promo_entries:
                for chunk_idx in range(0, len(promo_entries), 10):
                    chunk = promo_entries[chunk_idx:chunk_idx + 10]
                    file_num = chunk_idx // 10 + 1

                    filename = f'telegram_{category}_promo_{file_num}.txt'
                    filepath = promo_dir / filename

                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(f"# –ü—Ä–∏–º–µ—Ä—ã –ø–æ—Å—Ç–æ–≤: {title_map.get(category, category)}\n\n")
                        f.write(f"–ò—Å—Ç–æ—á–Ω–∏–∫: –†–∞–±–æ—á–∏–π –∫–∞–Ω–∞–ª NL International\n")
                        f.write(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}\n")
                        f.write(f"–¢–∏–ø: PROMO (–ø—Ä–∏–º–µ—Ä—ã —Å —Ü–µ–Ω–∞–º–∏/–∞–∫—Ü–∏—è–º–∏)\n")
                        f.write(f"‚ö†Ô∏è –í–ê–ñ–ù–û: –¶–µ–Ω—ã –∏ –∞–∫—Ü–∏–∏ –≤ —ç—Ç–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –£–°–¢–ê–†–ï–õ–ò!\n")
                        f.write(f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –∫–∞–∫ –®–ê–ë–õ–û–ù–´ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ—Å—Ç–æ–≤.\n\n")
                        f.write("---\n\n")

                        for entry in chunk:
                            f.write(f"[–ü—Ä–∏–º–µ—Ä –æ—Ç {entry['date'][:10]}]\n\n")
                            f.write(f"{entry['text']}\n\n")
                            f.write("---\n\n")

                    created_promo += 1

        print(f"\n–°–æ–∑–¥–∞–Ω–æ EVERGREEN –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {created_evergreen}")
        print(f"  –ü–∞–ø–∫–∞: {rag_dir}")
        print(f"\n–°–æ–∑–¥–∞–Ω–æ PROMO –ø—Ä–∏–º–µ—Ä–æ–≤: {created_promo}")
        print(f"  –ü–∞–ø–∫–∞: {promo_dir}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é
        print("\n--- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é ---")
        for category, entries in self.extracted_content.items():
            if not entries:
                continue
            evergreen_count = sum(1 for e in entries if not e['is_time_sensitive'])
            promo_count = sum(1 for e in entries if e['is_time_sensitive'])
            print(f"  {category}: {evergreen_count} evergreen, {promo_count} promo")

        return rag_dir

    def run(self):
        """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
        print("\n" + "=" * 60)
        print("–ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ó–ù–ê–ù–ò–ô –ò–ó TELEGRAM –≠–ö–°–ü–û–†–¢–ê")
        print("=" * 60)
        print(f"–ü—É—Ç—å: {self.export_path}\n")

        if not self.load_export():
            return False

        self.process_messages()
        self.deduplicate_content()
        output_dir = self.save_results()
        self.create_rag_documents(output_dir)

        print("\n" + "=" * 60)
        print("–ì–û–¢–û–í–û!")
        print("=" * 60)
        print("\n–ß—Ç–æ –¥–∞–ª—å—à–µ:")
        print("  1. –ü—Ä–æ–≤–µ—Ä—å –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: content/telegram_knowledge/")
        print("  2. RAG –¥–æ–∫—É–º–µ–Ω—Ç—ã: content/knowledge_base/from_telegram/")
        print("  3. –ó–∞–ø—É—Å—Ç–∏ load_knowledge_base.py –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")

        return True


def main():
    import sys

    if len(sys.argv) > 1:
        export_path = sys.argv[1]
    else:
        export_path = input("–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —ç–∫—Å–ø–æ—Ä—Ç–æ–º Telegram: ").strip('"')

    if not Path(export_path).exists():
        print(f"[ERROR] –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {export_path}")
        return

    extractor = TelegramKnowledgeExtractor(export_path)
    extractor.run()


if __name__ == "__main__":
    main()
