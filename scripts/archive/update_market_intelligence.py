#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.

–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —Ä–∞–∑ –≤ –¥–µ–Ω—å (—á–µ—Ä–µ–∑ cron/systemd timer).
–°–æ–±–∏—Ä–∞–µ—Ç:
- –ù–æ–≤–æ—Å—Ç–∏ NL International
- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö
- –¢—Ä–µ–Ω–¥—ã –≤ –Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥–∏–∏ –∏ MLM
- –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –∞–∫—Ü–∏–∏ –∏ –ø—Ä–æ–º–æ

–î–æ–±–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ RAG –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –æ —Å–≤–µ–∂–µ—Å—Ç–∏.
"""

import asyncio
import logging
from datetime import datetime
from typing import List, Dict
import sys
from pathlib import Path
import aiohttp
from bs4 import BeautifulSoup
import re

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.append(str(Path(__file__).parent.parent))

from shared.config.settings import settings
from shared.rag.vector_store import VectorStore

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/market_intelligence.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class MarketIntelligenceCollector:
    """–°–±–æ—Ä—â–∏–∫ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""

    def __init__(self):
        self.vector_store = VectorStore()
        self.today = datetime.now().strftime("%Y-%m-%d")

        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        self.sources = {
            "nl_official": {
                "name": "NL International –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç",
                "url": "https://nl-international.ru/news/",
                "enabled": True
            },
            "competitors": [
                {
                    "name": "Herbalife Russia",
                    "keywords": ["herbalife", "–≥–µ—Ä–±–∞–ª–∞–π—Ñ"],
                    "enabled": True
                },
                {
                    "name": "Siberian Wellness",
                    "keywords": ["siberian wellness", "—Å–∏–±–∏—Ä—Å–∫–æ–µ –∑–¥–æ—Ä–æ–≤—å–µ"],
                    "enabled": True
                },
                {
                    "name": "Oriflame Wellness",
                    "keywords": ["oriflame wellness", "–æ—Ä–∏—Ñ–ª–µ–π–º"],
                    "enabled": True
                }
            ],
            "trends": [
                "–Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥–∏—è —Ç—Ä–µ–Ω–¥—ã 2026",
                "—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –ø–∏—Ç–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏",
                "—Å–µ—Ç–µ–≤–æ–π –º–∞—Ä–∫–µ—Ç–∏–Ω–≥ —Ä–æ—Å—Å–∏—è",
                "–ë–ê–î—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"
            ]
        }

    async def collect_all(self) -> Dict[str, List[str]]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

        Returns:
            Dict —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        """
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ —Å–±–æ—Ä–∞ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ {self.today}")

        results = {
            "nl_news": [],
            "competitor_insights": [],
            "industry_trends": [],
            "errors": []
        }

        try:
            # 1. –ù–æ–≤–æ—Å—Ç–∏ NL International
            logger.info("üì∞ –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π NL International...")
            nl_news = await self._collect_nl_news()
            results["nl_news"] = nl_news

            # 2. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
            logger.info("üîç –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤...")
            competitor_data = await self._collect_competitor_insights()
            results["competitor_insights"] = competitor_data

            # 3. –û—Ç—Ä–∞—Å–ª–µ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã
            logger.info("üìä –°–±–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏...")
            trends = await self._collect_industry_trends()
            results["industry_trends"] = trends

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            results["errors"].append(str(e))

        return results

    async def _collect_nl_news(self) -> List[str]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞ NL"""
        news_items = []

        try:
            url = self.sources["nl_official"]["url"]

            async with aiohttp.ClientSession() as session:
                try:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                        if response.status != 200:
                            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É NL: —Å—Ç–∞—Ç—É—Å {response.status}")
                            return self._get_nl_news_fallback()

                        html = await response.text()
                        soup = BeautifulSoup(html, 'lxml')

                        # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                        news_blocks = soup.find_all(['article', 'div'], class_=re.compile(r'news|article|post', re.I), limit=10)

                        if not news_blocks:
                            news_blocks = soup.find_all(['div'], class_=re.compile(r'item|card|entry', re.I), limit=10)

                        for block in news_blocks:
                            try:
                                title_elem = block.find(['h1', 'h2', 'h3', 'h4', 'a'])
                                title = title_elem.get_text(strip=True) if title_elem else ""

                                text_block = block.get_text(separator=' ', strip=True)
                                if title:
                                    text_block = text_block.replace(title, '', 1)

                                description = text_block[:500].strip() if text_block else ""

                                link_elem = block.find('a', href=True)
                                link = ""
                                if link_elem:
                                    href = link_elem['href']
                                    if href.startswith('/'):
                                        link = f"https://nl-international.ru{href}"
                                    elif href.startswith('http'):
                                        link = href

                                if title and len(title) > 10:
                                    news_doc = f"""
[–ù–û–í–û–°–¢–¨ NL] {self.today}
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}

{description}

–ò—Å—Ç–æ—á–Ω–∏–∫: NL International
URL: {link if link else 'https://nl-international.ru/news/'}
–ö–∞—Ç–µ–≥–æ—Ä–∏—è: –ù–æ–≤–æ—Å—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–∏
–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: –í—ã—Å–æ–∫–∞—è
                                    """.strip()

                                    news_items.append(news_doc)

                            except Exception as e:
                                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–ª–æ–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                                continue

                        if not news_items:
                            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ HTML, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                            return self._get_nl_news_fallback()

                except asyncio.TimeoutError:
                    logger.warning("‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã NL, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    return self._get_nl_news_fallback()
                except aiohttp.ClientError as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ NL: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    return self._get_nl_news_fallback()

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –Ω–æ–≤–æ—Å—Ç–µ–π NL: {e}")
            return self._get_nl_news_fallback()

        logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π NL")
        return news_items

    def _get_nl_news_fallback(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è"""
        return [
            f"""
[–ù–û–í–û–°–¢–¨ NL] {self.today}
–ò—Å—Ç–æ—á–Ω–∏–∫: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç NL International

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.
–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç nl-international.ru/news/

–ö–∞—Ç–µ–≥–æ—Ä–∏—è: –ù–æ–≤–æ—Å—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–∏
–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: –ù–∏–∑–∫–∞—è (fallback)
            """.strip()
        ]

    async def _collect_competitor_insights(self) -> List[str]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö"""
        insights = []

        async with aiohttp.ClientSession() as session:
            for competitor in self.sources["competitors"]:
                if not competitor["enabled"]:
                    continue

                try:
                    competitor_name = competitor["name"]
                    keywords = competitor["keywords"]

                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –Ø–Ω–¥–µ–∫—Å –ù–æ–≤–æ—Å—Ç–µ–π
                    search_query = "+".join(keywords[:2])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
                    search_url = f"https://yandex.ru/news/search?text={search_query}"

                    try:
                        async with session.get(
                            search_url,
                            timeout=aiohttp.ClientTimeout(total=20),
                            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                        ) as response:
                            if response.status == 200:
                                html = await response.text()
                                soup = BeautifulSoup(html, 'lxml')

                                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
                                news_titles = []
                                for title_elem in soup.find_all(['h2', 'h3', 'a'], limit=5):
                                    title_text = title_elem.get_text(strip=True)
                                    if title_text and len(title_text) > 20:
                                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
                                        if any(kw.lower() in title_text.lower() for kw in keywords):
                                            news_titles.append(title_text)

                                if news_titles:
                                    news_summary = "\n- ".join(news_titles[:3])
                                    insight = f"""
[–ö–û–ù–ö–£–†–ï–ù–¢: {competitor_name}] {self.today}

–ü–æ—Å–ª–µ–¥–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö:
- {news_summary}

–ê–Ω–∞–ª–∏–∑: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ MLM –Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥–∏–∏.
–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(keywords)}

–ö–∞—Ç–µ–≥–æ—Ä–∏—è: –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: –°—Ä–µ–¥–Ω—è—è
                                    """.strip()
                                else:
                                    insight = self._get_competitor_fallback(competitor_name, keywords)

                            else:
                                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ {competitor_name}: —Å—Ç–∞—Ç—É—Å {response.status}")
                                insight = self._get_competitor_fallback(competitor_name, keywords)

                    except (asyncio.TimeoutError, aiohttp.ClientError) as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –æ {competitor_name}: {e}")
                        insight = self._get_competitor_fallback(competitor_name, keywords)

                    insights.append(insight)

                except Exception as e:
                    logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞ {competitor.get('name', 'unknown')}: {e}")
                    continue

        logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(insights)} –∏–Ω—Å–∞–π—Ç–æ–≤ –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö")
        return insights if insights else [self._get_competitor_fallback("–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑", [])]

    def _get_competitor_fallback(self, competitor_name: str, keywords: List[str]) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞"""
        return f"""
[–ö–û–ù–ö–£–†–ï–ù–¢: {competitor_name}] {self.today}

–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ MLM –Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥–∏–∏.
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.

–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å: {', '.join(keywords) if keywords else '–Ω–æ–≤–æ—Å—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–∏'}

–ö–∞—Ç–µ–≥–æ—Ä–∏—è: –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: –ù–∏–∑–∫–∞—è (fallback)
        """.strip()

    async def _collect_industry_trends(self) -> List[str]:
        """–°–æ–±–∏—Ä–∞–µ—Ç —Ç—Ä–µ–Ω–¥—ã –∏–Ω–¥—É—Å—Ç—Ä–∏–∏"""
        trends = []

        async with aiohttp.ClientSession() as session:
            for trend_query in self.sources["trends"]:
                try:
                    # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –Ø–Ω–¥–µ–∫—Å –ù–æ–≤–æ—Å—Ç—è—Ö
                    search_query = trend_query.replace(" ", "+")
                    search_url = f"https://yandex.ru/news/search?text={search_query}"

                    try:
                        async with session.get(
                            search_url,
                            timeout=aiohttp.ClientTimeout(total=20),
                            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                        ) as response:
                            if response.status == 200:
                                html = await response.text()
                                soup = BeautifulSoup(html, 'lxml')

                                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
                                trend_headlines = []
                                for elem in soup.find_all(['h2', 'h3', 'a'], limit=10):
                                    headline = elem.get_text(strip=True)
                                    if headline and len(headline) > 25 and len(headline) < 200:
                                        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                                        query_keywords = trend_query.lower().split()
                                        if any(word in headline.lower() for word in query_keywords[:3]):
                                            trend_headlines.append(headline)

                                if trend_headlines:
                                    # –ë–µ—Ä–µ–º —Ç–æ–ø-3 –∑–∞–≥–æ–ª–æ–≤–∫–∞
                                    top_headlines = trend_headlines[:3]
                                    headlines_text = "\n- ".join(top_headlines)

                                    trend = f"""
[–¢–†–ï–ù–î: {trend_query}] {self.today}

–ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–µ–º–µ:
- {headlines_text}

–ê–Ω–∞–ª–∏–∑: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥–∏–∏ –∏ —Å–µ—Ç–µ–≤–æ–º –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–µ.
–ò—Å—Ç–æ—á–Ω–∏–∫: –Ø–Ω–¥–µ–∫—Å –ù–æ–≤–æ—Å—Ç–∏

–ö–∞—Ç–µ–≥–æ—Ä–∏—è: –û—Ç—Ä–∞—Å–ª–µ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã
–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: –°—Ä–µ–¥–Ω—è—è
                                    """.strip()
                                else:
                                    trend = self._get_trend_fallback(trend_query)

                            else:
                                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–Ω–¥—ã –¥–ª—è '{trend_query}': —Å—Ç–∞—Ç—É—Å {response.status}")
                                trend = self._get_trend_fallback(trend_query)

                    except (asyncio.TimeoutError, aiohttp.ClientError) as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è '{trend_query}': {e}")
                        trend = self._get_trend_fallback(trend_query)

                    trends.append(trend)

                except Exception as e:
                    logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Ç—Ä–µ–Ω–¥–∞ '{trend_query}': {e}")
                    trends.append(self._get_trend_fallback(trend_query))
                    continue

        logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(trends)} —Ç—Ä–µ–Ω–¥–æ–≤")
        return trends if trends else [self._get_trend_fallback("–û–±—â–∏–µ —Ç—Ä–µ–Ω–¥—ã")]

    def _get_trend_fallback(self, trend_query: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–µ–Ω–¥–∞"""
        return f"""
[–¢–†–ï–ù–î: {trend_query}] {self.today}

–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥–∏–∏ –∏ —Å–µ—Ç–µ–≤–æ–º –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–µ.
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.

–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –ø–æ —Ç–µ–º–µ: {trend_query}

–ö–∞—Ç–µ–≥–æ—Ä–∏—è: –û—Ç—Ä–∞—Å–ª–µ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã
–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: –ù–∏–∑–∫–∞—è (fallback)
        """.strip()

    async def save_to_knowledge_base(self, data: Dict[str, List[str]]) -> int:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ RAG –±–∞–∑—É –∑–Ω–∞–Ω–∏–π

        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –∏ —Å–ø–∏—Å–∫–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Returns:
            int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        total_saved = 0

        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            await self.vector_store.init_tables()

            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ RAG –±–∞–∑—É...")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ NL
            for doc in data.get('nl_news', []):
                doc_id = await self.vector_store.add_document(
                    content=doc,
                    source="NL International",
                    category="market_intelligence_nl_news",
                    metadata={
                        "date": self.today,
                        "type": "nl_news",
                        "relevance": "high",
                        "auto_collected": True
                    }
                )
                if doc_id:
                    total_saved += 1
                    logger.debug(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å NL (ID: {doc_id})")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Å–∞–π—Ç—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
            for doc in data.get('competitor_insights', []):
                doc_id = await self.vector_store.add_document(
                    content=doc,
                    source="Competitor Analysis",
                    category="market_intelligence_competitors",
                    metadata={
                        "date": self.today,
                        "type": "competitor_insight",
                        "relevance": "medium",
                        "auto_collected": True
                    }
                )
                if doc_id:
                    total_saved += 1
                    logger.debug(f"‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω –∏–Ω—Å–∞–π—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞ (ID: {doc_id})")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–µ–Ω–¥—ã
            for doc in data.get('industry_trends', []):
                doc_id = await self.vector_store.add_document(
                    content=doc,
                    source="Industry Trends",
                    category="market_intelligence_trends",
                    metadata={
                        "date": self.today,
                        "type": "industry_trend",
                        "relevance": "medium",
                        "auto_collected": True
                    }
                )
                if doc_id:
                    total_saved += 1
                    logger.debug(f"‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω —Ç—Ä–µ–Ω–¥ (ID: {doc_id})")

            logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_saved} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ RAG –±–∞–∑—É")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π: {e}", exc_info=True)

        return total_saved

    async def cleanup_old_data(self, days_to_keep: int = 30):
        """
        –£–¥–∞–ª—è–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

        Args:
            days_to_keep: –°–∫–æ–ª—å–∫–æ –¥–Ω–µ–π —Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
        """
        from datetime import timedelta
        from sqlalchemy import delete

        logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ä—à–µ {days_to_keep} –¥–Ω–µ–π...")

        try:
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ market_intelligence
            from shared.rag.vector_store import Document
            from shared.database.base import AsyncSessionLocal

            async with AsyncSessionLocal() as session:
                # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π market_intelligence —Å—Ç–∞—Ä—à–µ cutoff_date
                stmt = delete(Document).where(
                    Document.category.like('market_intelligence%')
                ).where(
                    Document.created_at < cutoff_date
                )

                result = await session.execute(stmt)
                await session.commit()

                deleted_count = result.rowcount
                logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {deleted_count} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ: {e}", exc_info=True)

    async def generate_summary(self, data: Dict[str, List[str]]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç –æ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –∏ —Å–ø–∏—Å–∫–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Returns:
            str: –¢–µ–∫—Å—Ç –æ—Ç—á–µ—Ç–∞
        """
        summary = f"""
        üìä –û–¢–ß–ï–¢ –û –ú–ê–†–ö–ï–¢–ò–ù–ì–û–í–û–ô –ê–ù–ê–õ–ò–¢–ò–ö–ï
        –î–∞—Ç–∞: {self.today}

        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        üì∞ –ù–æ–≤–æ—Å—Ç–∏ NL International: {len(data.get('nl_news', []))}
        üîç –ò–Ω—Å–∞–π—Ç—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {len(data.get('competitor_insights', []))}
        üìä –û—Ç—Ä–∞—Å–ª–µ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã: {len(data.get('industry_trends', []))}

        ‚ùå –û—à–∏–±–∫–∏: {len(data.get('errors', []))}

        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {sum(len(v) for k, v in data.items() if k != 'errors')}
        """.strip()

        if data.get('errors'):
            summary += "\n\n‚ö†Ô∏è –û–®–ò–ë–ö–ò:\n"
            for error in data['errors']:
                summary += f"- {error}\n"

        return summary


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        collector = MarketIntelligenceCollector()

        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = await collector.collect_all()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        saved_count = await collector.save_to_knowledge_base(data)

        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
        await collector.cleanup_old_data(days_to_keep=30)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        summary = await collector.generate_summary(data)
        logger.info(f"\n{summary}")

        logger.info("‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
